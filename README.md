# ğŸš€ DE Project 360 â€“ End-to-End Spark Data Platform

## ğŸ“Œ Overview

**DE Project 360** is a production-style **Data Engineering platform** built using **Apache Spark, Docker, and Jupyter**, following the **Medallion Architecture (Bronzeâ€“Silverâ€“Gold)** pattern. The project demonstrates how raw data is ingested, cleaned, validated, enriched, aggregated, monitored, and analyzed with **ML-based anomaly detection** and **interactive dashboards**.

This repository is designed to mirror **real-world data engineering systems**, not just scripts.

---

## ğŸ§± Architecture

```
Raw CSV Data
     â†“
Bronze Layer  (Raw â†’ Parquet)
     â†“
Silver Layer  (Clean + Validate + Quarantine)
     â†“
Gold Layer    (Business Aggregations)
     â†“
Semantic Layer (Business Metrics)
     â†“
Analytics / ML / Dashboards
```

---

## ğŸ›  Tech Stack

* **Apache Spark 3.5** (PySpark)
* **Docker & Docker Compose**
* **JupyterLab** (Spark UI enabled)
* **Plotly** (Dashboards)
* **Spark MLlib** (Anomaly Detection)
* **Git & GitHub**

---

## ğŸ“‚ Project Structure

```
DE PROJECT 360/
â”œâ”€â”€ docker/
â”‚   â””â”€â”€ Dockerfile
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ data/
â”‚   â””â”€â”€ raw/
â”‚       â””â”€â”€ client_sales_raw.csv
â”œâ”€â”€ jobs/
â”‚   â”œâ”€â”€ bronze/
â”‚   â”‚   â””â”€â”€ bronze_layer.py
â”‚   â”œâ”€â”€ silver/
â”‚   â”‚   â””â”€â”€ silver_layer.py
â”‚   â”œâ”€â”€ gold/
â”‚   â”‚   â””â”€â”€ gold_layer.py
â”‚   â”œâ”€â”€ observability/
â”‚   â”‚   â””â”€â”€ observability.py
â”‚   â”œâ”€â”€ semantic/
â”‚   â”‚   â””â”€â”€ semantic_layer.py
â”‚   â””â”€â”€ ai/
â”‚       â””â”€â”€ anomaly_detection.py
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ dashboards/
â”‚   â”‚   â””â”€â”€ sales_kpis_plotly.ipynb
â”‚   â””â”€â”€ ml/
â”‚       â””â”€â”€ anomaly_detection.ipynb
â””â”€â”€ README.md
```

---

## ğŸ¥‰ Bronze Layer â€“ Raw Ingestion

Reads raw CSV data and stores it as immutable Parquet files.

```python
from pyspark.sql import SparkSession

spark = SparkSession.builder.appName("BronzeLayer").getOrCreate()

df = spark.read.option("header", True) \
    .csv("/opt/data/data/raw/client_sales_raw.csv")

df.write.mode("overwrite") \
    .parquet("/opt/data/bronze/sales_transactions")

spark.stop()
```

---

## ğŸ¥ˆ Silver Layer â€“ Data Cleaning & Validation

* Casts data types
* Filters invalid records
* Quarantines bad data

```python
from pyspark.sql import SparkSession
from pyspark.sql.functions import col

spark = SparkSession.builder.appName("SilverLayer").getOrCreate()

df = spark.read.parquet("/opt/data/bronze/sales_transactions")

clean_df = df \
    .withColumn("amount", col("amount").cast("double")) \
    .filter(col("amount") > 0)

clean_df.write.mode("overwrite") \
    .parquet("/opt/data/silver/sales_clean")

(df.subtract(clean_df)) \
    .write.mode("overwrite") \
    .parquet("/opt/data/silver/sales_quarantine")

spark.stop()
```

---

## ğŸ¥‡ Gold Layer â€“ Business Aggregations

Creates KPI-ready datasets.

```python
from pyspark.sql import SparkSession
from pyspark.sql.functions import sum

spark = SparkSession.builder.appName("GoldLayer").getOrCreate()

df = spark.read.parquet("/opt/data/silver/sales_clean")

gold_df = df.groupBy("status") \
    .agg(sum("amount").alias("total_sales"))

gold_df.write.mode("overwrite") \
    .parquet("/opt/data/gold/sales_summary")

spark.stop()
```

---

## ğŸ§  Semantic Layer

Provides business-friendly metrics.

```python
from pyspark.sql import SparkSession

spark = SparkSession.builder.appName("SemanticLayer").getOrCreate()

df = spark.read.parquet("/opt/data/gold/sales_summary")

df.write.mode("overwrite") \
    .parquet("/opt/data/semantic/business_metrics")

spark.stop()
```

---

## ğŸ” Observability Layer

Tracks pipeline health and row counts.

```python
from pyspark.sql import SparkSession
from datetime import datetime

spark = SparkSession.builder.appName("Observability").getOrCreate()

metrics = [(1000, True, datetime.now())]

df = spark.createDataFrame(metrics, ["row_count", "success", "timestamp"])

df.write.mode("append") \
    .parquet("/opt/data/observability/pipeline_metrics")

spark.stop()
```

---

## ğŸ¤– AI / ML â€“ Anomaly Detection

Uses Spark MLlib to detect unusual sales behavior.

```python
from pyspark.ml.feature import VectorAssembler
from pyspark.ml.clustering import KMeans
from pyspark.sql.functions import col

assembler = VectorAssembler(inputCols=["amount"], outputCol="features")
data = assembler.transform(df)

kmeans = KMeans(k=2, seed=42)
model = kmeans.fit(data)

predictions = model.transform(data)
```

---

## ğŸ“Š Dashboards

Interactive dashboards built using **Plotly + Jupyter**:

* Total sales over time
* Clean vs quarantined records
* Customer-level sales

Notebook: `notebooks/dashboards/sales_kpis_plotly.ipynb`

---

## â–¶ï¸ How to Run

```bash
docker-compose up --build
```

Open:

* JupyterLab â†’ [http://localhost:8888](http://localhost:8888)
* Spark UI â†’ [http://localhost:4040](http://localhost:4040) (when a job runs)

---

## ğŸ¯ What This Project Demonstrates

* Real-world Spark pipeline design
* Medallion Architecture
* Data quality & observability
* ML integration in data platforms
* Analytics-ready outputs

---
### ğŸ§ª Data Quality â€“ Clean vs Quarantined Records

![Clean vs Quarantined Records](Images/clean_vs_quarantined_raw.png)

This visualization shows the comparison between valid (clean) records and quarantined records that failed business or data quality rules.

### ğŸ” Pipeline Data Quality & Observability

![Pipeline Data Quality](Images/Pipeline_data_quality.png)

Tracks row counts, pipeline success status, and overall data health across Bronze, Silver, and Gold layers.

### ğŸ¤– ML-Based Sales Clustering

![Sales Clustering](Images/Sales_clustering.png)

KMeans clustering applied on sales amounts to identify anomalies and unusual transaction patterns.

### ğŸ‘¥ Total Sales by Customer

![Total Sales by Customer](Images/Total_sales_by_customers.png)

Highlights high-value customers and customer-level revenue contribution.

### ğŸ“ˆ Total Sales by Transaction Status

![Total Sales by Status](Images/Total_sales_by_status.png)

Summarizes business performance based on transaction status (e.g., completed, pending, failed).


## ğŸ‘¤ Author

**Muhammed Fathah**
Data Engineer | Apache Spark | Docker | Big Data

---

â­ If you find this project useful, give it a star!
